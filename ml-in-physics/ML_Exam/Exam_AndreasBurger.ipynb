{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Andreas Burger\n",
    "Exam ID: 37\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import (BatchNormalization, Conv2D, Conv2DTranspose, Dense,\n",
    "                                            Dropout, Flatten, Multiply, Add, Input,\n",
    "                                            Conv2D, Activation, BatchNormalization, MaxPool2D,\n",
    "                                            UpSampling2D, Conv2DTranspose,\n",
    "                                            LeakyReLU, Reshape)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.backend import random_normal\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time\n",
    "#import tensorflow_probability as tfp\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# 3\n",
    "\n",
    "\"\"\"\n",
    "How can the hidden dense layer be modified such that the weights are initialised\n",
    "with a Gaussian with mean zero and standard deviation 100? Provide your answer in python\n",
    "code which in principle can be executed.\n",
    "\"\"\"\n",
    "\n",
    "initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=100.)\n",
    "layer = Dense(3, kernel_initializer=initializer)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_65 (InputLayer)        [(None, 40, 40)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_59 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_471 (Dense)            (None, 10)                16000     \n",
      "_________________________________________________________________\n",
      "afterinput (Dense)           (None, 1000)              10000     \n",
      "_________________________________________________________________\n",
      "bottleneck (Dense)           (None, 10)                10000     \n",
      "_________________________________________________________________\n",
      "dense_472 (Dense)            (None, 100)               1000      \n",
      "_________________________________________________________________\n",
      "dense_473 (Dense)            (None, 10)                1000      \n",
      "_________________________________________________________________\n",
      "dense_474 (Dense)            (None, 100)               1000      \n",
      "_________________________________________________________________\n",
      "dense_475 (Dense)            (None, 400)               40000     \n",
      "_________________________________________________________________\n",
      "dense_476 (Dense)            (None, 10)                4000      \n",
      "_________________________________________________________________\n",
      "dense_477 (Dense)            (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_478 (Dense)            (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_479 (Dense)            (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_480 (Dense)            (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_481 (Dense)            (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_482 (Dense)            (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_483 (Dense)            (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_484 (Dense)            (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_485 (Dense)            (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_486 (Dense)            (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_487 (Dense)            (None, 1600)              16000     \n",
      "=================================================================\n",
      "Total params: 100,000\n",
      "Trainable params: 100,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "\n",
    "\"\"\"\n",
    "(9 points) Write down an autoencoder using Keras which can be used for 2D Ising spin configu-\n",
    "rations (40x40). Your network has to have 100 000 trainable parameters. Submit your network\n",
    "as an executable code. (Hint: Layers without a bias might render matching the exact number\n",
    "of parameters easier.)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(Flatten)\n",
    "autoencoder.add(Dense(100,  activation='elu', use_bias=False, input_shape=(40*40)))\n",
    "autoencoder.add(Dense(100,  activation='elu', use_bias=False))\n",
    "autoencoder.add(Dense(10,    activation='linear', use_bias=False, name=\"bottleneck\"))\n",
    "autoencoder.add(Dense(100,  activation='elu', use_bias=False))\n",
    "autoencoder.add(Dense(100,  activation='elu', use_bias=False))\n",
    "autoencoder.add(Dense(40*40,  activation='sigmoid', use_bias=False))\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer = Adam())\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "input_network = Input(shape=(40,40))\n",
    "x1 = Flatten()(input_network)\n",
    "x = Dense(10, activation='relu', use_bias=False)(x1)\n",
    "x = Dense(1000, activation='relu', use_bias=False, name=\"afterinput\")(x)\n",
    "x = Dense(10, activation='linear', use_bias=False, name=\"bottleneck\")(x)\n",
    "x = Dense(100, activation='relu', use_bias=False)(x)\n",
    "x = Dense(10, activation='relu', use_bias=False)(x)\n",
    "x = Dense(100, activation='relu', use_bias=False)(x)\n",
    "x = Dense(400, activation='relu', use_bias=False)(x)\n",
    "x = Dense(10, activation='relu', use_bias=False)(x)\n",
    "x = Dense(10, activation='relu', use_bias=False)(x)\n",
    "x = Dense(10, activation='relu', use_bias=False)(x)\n",
    "x = Dense(10, activation='relu', use_bias=False)(x)\n",
    "x = Dense(10, activation='relu', use_bias=False)(x)\n",
    "x = Dense(10, activation='relu', use_bias=False)(x)\n",
    "x = Dense(10, activation='relu', use_bias=False)(x)\n",
    "x = Dense(10, activation='relu', use_bias=False)(x)\n",
    "x = Dense(10, activation='relu', use_bias=False)(x)\n",
    "x = Dense(10, activation='relu', use_bias=False)(x)\n",
    "x = Dense(10, activation='relu', use_bias=False)(x)\n",
    "output = Dense(40*40, activation='softmax', use_bias=False)(x)\n",
    "\n",
    "autoencoder = Model(input_network,outputs=output, name=\"autoencoder\")\n",
    "\n",
    "\n",
    "autoencoder.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# 6 GAN\n",
    "\n",
    "\"\"\"\n",
    "Provide an executable python implementation of the training step where you make\n",
    "two gradient updates on the training data for the generator when the generator is only updated\n",
    "once.\n",
    "\"\"\"\n",
    "latent_dim = 10\n",
    "Batch_Size = 50\n",
    "\n",
    "generator = Sequential(name=\"generator\")\n",
    "generator.add(Dense(10, use_bias=False, input_shape=(latent_dim,)))\n",
    "\n",
    "discriminator = Sequential(name=\"discriminator\")\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    gl = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    return gl\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "def train_step(images):\n",
    "    latent_vector = tf.random.normal([Batch_Size, latent_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(latent_vector, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "    # second time for generator only\n",
    "    latent_vector = tf.random.normal([Batch_Size, latent_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(latent_vector, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      #disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    #gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    #discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [1,100], In[1]: [1,100] [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-80-f8aa69f39f7d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mGradientTape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mt3\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mGradientTape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mt1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 21\u001B[1;33m         \u001B[0mu\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m \u001B[1;33m,\u001B[0m\u001B[0mtraining\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     22\u001B[0m     \u001B[0mdu_dx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mt1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgradient\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mu\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    983\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    984\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menable_auto_cast_variables\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_compute_dtype_object\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 985\u001B[1;33m           \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    986\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    987\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_activity_regularizer\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, inputs, training, mask)\u001B[0m\n\u001B[0;32m    383\u001B[0m         \u001B[0ma\u001B[0m \u001B[0mlist\u001B[0m \u001B[0mof\u001B[0m \u001B[0mtensors\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mthere\u001B[0m \u001B[0mare\u001B[0m \u001B[0mmore\u001B[0m \u001B[0mthan\u001B[0m \u001B[0mone\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    384\u001B[0m     \"\"\"\n\u001B[1;32m--> 385\u001B[1;33m     return self._run_internal_graph(\n\u001B[0m\u001B[0;32m    386\u001B[0m         inputs, training=training, mask=mask)\n\u001B[0;32m    387\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001B[0m in \u001B[0;36m_run_internal_graph\u001B[1;34m(self, inputs, training, mask)\u001B[0m\n\u001B[0;32m    506\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    507\u001B[0m         \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnode\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap_arguments\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor_dict\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 508\u001B[1;33m         \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnode\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    509\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    510\u001B[0m         \u001B[1;31m# Update tensor_dict.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    983\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    984\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menable_auto_cast_variables\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_compute_dtype_object\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 985\u001B[1;33m           \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    986\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    987\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_activity_regularizer\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m   1191\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1192\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mcall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1193\u001B[1;33m     return core_ops.dense(\n\u001B[0m\u001B[0;32m   1194\u001B[0m         \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1195\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkernel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\ops\\core.py\u001B[0m in \u001B[0;36mdense\u001B[1;34m(inputs, kernel, bias, activation, dtype)\u001B[0m\n\u001B[0;32m     54\u001B[0m   \u001B[1;31m# Broadcast kernel to inputs.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m   \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 56\u001B[1;33m     \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstandard_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensordot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkernel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mrank\u001B[0m \u001B[1;33m-\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     57\u001B[0m     \u001B[1;31m# Reshape the output back to the original ndim of the input.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    199\u001B[0m     \u001B[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    200\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 201\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    202\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    203\u001B[0m       \u001B[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001B[0m in \u001B[0;36mtensordot\u001B[1;34m(a, b, axes, name)\u001B[0m\n\u001B[0;32m   4517\u001B[0m     b_reshape, b_free_dims, b_free_dims_static = _tensordot_reshape(\n\u001B[0;32m   4518\u001B[0m         b, b_axes, True)\n\u001B[1;32m-> 4519\u001B[1;33m     \u001B[0mab_matmul\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmatmul\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma_reshape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb_reshape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4520\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma_free_dims\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb_free_dims\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4521\u001B[0m       if (ab_matmul.get_shape().is_fully_defined() and\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    199\u001B[0m     \u001B[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    200\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 201\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    202\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    203\u001B[0m       \u001B[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001B[0m in \u001B[0;36mmatmul\u001B[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001B[0m\n\u001B[0;32m   3252\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0mret\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3253\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3254\u001B[1;33m       return gen_math_ops.mat_mul(\n\u001B[0m\u001B[0;32m   3255\u001B[0m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001B[0;32m   3256\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001B[0m in \u001B[0;36mmat_mul\u001B[1;34m(a, b, transpose_a, transpose_b, name)\u001B[0m\n\u001B[0;32m   5621\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5622\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 5623\u001B[1;33m       \u001B[0m_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraise_from_not_ok_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   5624\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5625\u001B[0m       \u001B[1;32mpass\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36mraise_from_not_ok_status\u001B[1;34m(e, name)\u001B[0m\n\u001B[0;32m   6841\u001B[0m   \u001B[0mmessage\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmessage\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m\" name: \"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;34m\"\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6842\u001B[0m   \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 6843\u001B[1;33m   \u001B[0msix\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraise_from\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_status_to_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmessage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   6844\u001B[0m   \u001B[1;31m# pylint: enable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6845\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\six.py\u001B[0m in \u001B[0;36mraise_from\u001B[1;34m(value, from_value)\u001B[0m\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m: Matrix size-incompatible: In[0]: [1,100], In[1]: [1,100] [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "# 7\n",
    "\"\"\"\n",
    "Implement a custom loss function which ensures that the following differential equa-\n",
    "tion is satisfied x' = squrt(1 + x^2) * x\n",
    "\"\"\"\n",
    "\n",
    "inp = Input(shape=(1,))\n",
    "x = Dense(100,activation='tanh')(inp)\n",
    "x = Dense(100,activation='tanh')(x)\n",
    "out = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=inp,outputs=out)\n",
    "\n",
    "\n",
    "train_data = np.random.normal(loc=0, scale=1, size=100) # gaussian\n",
    "train_data /= train_data.max()  # rescale to [-1,1] interval\n",
    "x = tf.Variable(train_data)\n",
    "\n",
    "with tf.GradientTape() as t3:\n",
    "    with tf.GradientTape() as t1:\n",
    "        u = model(x ,training=True)\n",
    "    du_dx = t1.gradient(u,x)\n",
    "\n",
    "    loss = (du_dx - (np.sqrt(1 + u**2) * u) )**2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a training step which uses this custom loss function evaluated with 1000\n",
    "uniformly sampled points in the interval (-1,1) for training.\n",
    "\"\"\"\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "\n",
    "inp = Input(shape=(1,))\n",
    "x = Dense(100,activation='tanh')(inp)\n",
    "x = Dense(100,activation='tanh')(x)\n",
    "out = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=inp,outputs=out)\n",
    "#model.summary()\n",
    "\n",
    "def train_step(samples):\n",
    "    train_data = np.random.uniform(-1,1,size=(samples,1))\n",
    "    #train_data /= train_data.max()  # rescale to [-1,1] interval\n",
    "    x = tf.Variable(train_data)\n",
    "\n",
    "    with tf.GradientTape() as t3:\n",
    "        with tf.GradientTape() as t1:\n",
    "            u = model(x ,training=True)\n",
    "        du_dx=t1.gradient(u,x)\n",
    "\n",
    "        # u''(x) + a u'(x) + b u(x) = 0\n",
    "        #loss = (d2u_dx + a*du_dx + b*u)**2\n",
    "\n",
    "        loss = (du_dx - (np.sqrt(1 + u**2) * u) )**2\n",
    "\n",
    "\n",
    "    gradients_of_nn = t3.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients_of_nn, model.trainable_variables))\n",
    "\n",
    "    print(np.mean(loss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a training loop where this training step is applied 100 times and output the\n",
    "training error for each training step.\n",
    "\"\"\"\n",
    "\n",
    "def train(epochs = 1):\n",
    "    for epoch in range(epochs):\n",
    "        train_step(100)\n",
    "\n",
    "#train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-c4f24b27",
   "language": "python",
   "display_name": "PyCharm (MLinPhysics)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}