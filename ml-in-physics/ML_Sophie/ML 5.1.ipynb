{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST â€“ Conv2D\n",
    " - compare the performance of convolutional neural networks vs fully connected networks.\n",
    "\n",
    "### Create new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # images are numpy arrays\n",
    "# import tensorflow as tf # needed for the MNIST data set, not used this time!\n",
    "\n",
    "from sklearn.datasets import fetch_openml # MNIST data\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, Dropout, Activation, MaxPooling2D\n",
    "# imports our layers we want to use\n",
    "\n",
    "from tensorflow.python.keras.losses import categorical_crossentropy #loss function\n",
    "from tensorflow.python.keras.optimizers import Adam, SGD #optimisers\n",
    "from tensorflow.keras.utils import to_categorical #some function for data preparation\n",
    "\n",
    "from tensorflow.compat.v1 import disable_eager_execution\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset was already used in Sheet 3, import dataset again, this time try method in sample solution\n",
    "# all in sklearn\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(70000, 784) (70000,)\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3.  18.\n",
      "  18.  18. 126. 136. 175.  26. 166. 255. 247. 127.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  30.  36.  94. 154. 170. 253.\n",
      " 253. 253. 253. 253. 225. 172. 253. 242. 195.  64.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  49. 238. 253. 253. 253. 253. 253.\n",
      " 253. 253. 253. 251.  93.  82.  82.  56.  39.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  18. 219. 253. 253. 253. 253. 253.\n",
      " 198. 182. 247. 241.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  80. 156. 107. 253. 253. 205.\n",
      "  11.   0.  43. 154.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.  14.   1. 154. 253.  90.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 139. 253. 190.\n",
      "   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  11. 190. 253.\n",
      "  70.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  35. 241.\n",
      " 225. 160. 108.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  81.\n",
      " 240. 253. 253. 119.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  45. 186. 253. 253. 150.  27.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.  16.  93. 252. 253. 187.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0. 249. 253. 249.  64.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  46. 130. 183. 253. 253. 207.   2.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  39. 148.\n",
      " 229. 253. 253. 253. 250. 182.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  24. 114. 221. 253.\n",
      " 253. 253. 253. 201.  78.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  23.  66. 213. 253. 253. 253.\n",
      " 253. 198.  81.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  18. 171. 219. 253. 253. 253. 253. 195.\n",
      "  80.   9.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.  55. 172. 226. 253. 253. 253. 253. 244. 133.  11.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 136. 253. 253. 253. 212. 135. 132.  16.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(type(X), type(y))\n",
    "print(np.shape(X), np.shape(y))\n",
    "\n",
    "# print one example \n",
    "print(X[0])\n",
    "\n",
    "plt.imshow(X[0].reshape(28,28))\n",
    "plt.show()\n",
    "\n",
    "# backround has to be simply zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape, place and reshape X\n",
    "# how efficient is this actually\n",
    "\n",
    "X_padded = np.zeros((70000,2500))\n",
    "for i in range(70000):\n",
    "    # reshape to square\n",
    "    square = X[i].reshape(28,28)\n",
    "    o = np.random.randint(22) #padding above, 50-28 = 22\n",
    "    u = 22-o\n",
    "    l = np.random.randint(22) #padding above, 50-28 = 22\n",
    "    r = 22-l\n",
    "    padded = np.pad(square, ((o,u), (l,r)), 'constant', constant_values=0)\n",
    "    X_padded[i] = padded.reshape(2500,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 2500)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 2500)\n"
     ]
    }
   ],
   "source": [
    "train_size = 60000\n",
    "test_size = 10000\n",
    "\n",
    "# divide into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, train_size=train_size, test_size=test_size)\n",
    "\n",
    "# this rescales the images; does not change the shape, only values of entries\n",
    "# scale data to have zero mean and unit variance [required by regressor]\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # fit AND transform\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(np.shape(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train simple dense neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# single hidden dense layer\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "# one-hot encode target column \n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sophie\\Anaconda3\\envs\\ml37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "disable_eager_execution() # this had to be added to avoid error message\n",
    "\n",
    "#Neural network with single dense hidden layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 158us/sample - loss: 2.9656 - accuracy: 0.0943\n",
      "\n",
      "Test loss: 2.965608910751343\n",
      "Test accuracy: 0.0943\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the accuracy is very low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement with convolutional neural network\n",
    "\n",
    "I decided to test a network with two convolutional layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "\n",
    "# shape that our model expects is different from what we have so far\n",
    "X_train = X_train.reshape(60000,50,50,1)\n",
    "X_test = X_test.reshape(10000,50,50,1)\n",
    "\n",
    "# first number: number of images (60,000 for X_train and 10,000 for X_test). \n",
    "# Then comes the shape of each image (28x28). \n",
    "# The last number is 1, which signifies that the images are greyscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "\n",
    "# input shape is shape of one image, one single array\n",
    "print(X_train.shape[1:])\n",
    "\n",
    "model = Sequential()\n",
    "# Sequential is the easiest way to build a model in Keras. \n",
    "# It allows you to build a model layer by layer.\n",
    "\n",
    "# first layer \n",
    "# maybe try size 64 later\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=X_train.shape[1:]))\n",
    "# 32 is output shape\n",
    "model.add(Activation('relu'))\n",
    "# Rectified Linear Activation. \n",
    "# This activation function has been proven to work well in neural networks.\n",
    "\n",
    "# second layer\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 322s 5ms/sample - loss: 1.4581 - accuracy: 0.5058 - val_loss: 1.0273 - val_accuracy: 0.6752\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 364s 6ms/sample - loss: 0.8865 - accuracy: 0.7294 - val_loss: 0.8287 - val_accuracy: 0.7542\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 357s 6ms/sample - loss: 0.7138 - accuracy: 0.7959 - val_loss: 0.7105 - val_accuracy: 0.7946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23344402fc8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 0.7105 - accuracy: 0.7946\n",
      "\n",
      "Test loss: 0.7104521778106689\n",
      "Test accuracy: 0.7946\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with only 3 epochs the accuracy is improved about ten times compared to the normal dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetuning: Improve network performance \n",
    "The changes I tried were\n",
    "- MaxPooling added as extra layer\n",
    "- exchanged one convolutional layer to MaxPooling \n",
    "- adding two more convolutional layers --> deeper architecture\n",
    "- dropout for both convolutional layers \n",
    "\n",
    "To compare the results just look at the accuracy of each of the trained network. \n",
    "Only a deeper architecture imporoves the result in my very simple natwork, but takes a lot longer. For the other changes I played around with the accuracy wasn't improved. Due to the long runtime I couldn't really investigate deeper networks or use a higher number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50, 1)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 495s 8ms/sample - loss: 1.1670 - accuracy: 0.6264 - val_loss: 0.7668 - val_accuracy: 0.7739\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 484s 8ms/sample - loss: 0.6062 - accuracy: 0.8294 - val_loss: 0.5348 - val_accuracy: 0.8552\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 481s 8ms/sample - loss: 0.4502 - accuracy: 0.8780 - val_loss: 0.4325 - val_accuracy: 0.8870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23344a58e08>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pooling, add one layer\n",
    "\n",
    "# In \"pool\" of the 3x3 matrix of the convolution the highest value is selected\n",
    "# same network as before but add pooling\n",
    "\n",
    "# build model\n",
    "\n",
    "# input shape is shape of one image, one single array\n",
    "print(X_train.shape[1:])\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# keep the first regular convolutional layer \n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# add one pooling layer \n",
    "model.add(MaxPooling2D(pool_size =(3, 3), padding='same', strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# normal convolutional layer\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# training the model\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50, 1)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 114s 2ms/sample - loss: 1.6913 - accuracy: 0.4393 - val_loss: 1.3612 - val_accuracy: 0.5967\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 108s 2ms/sample - loss: 1.1325 - accuracy: 0.6561 - val_loss: 1.0960 - val_accuracy: 0.6676\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 104s 2ms/sample - loss: 0.9535 - accuracy: 0.7136 - val_loss: 1.0008 - val_accuracy: 0.6941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x215b6233808>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pooling, replace one layer\n",
    "\n",
    "# In \"pool\" of the 3x3 matrix of the convolution the highest value is selected\n",
    "# same network as before but add pooling\n",
    "\n",
    "# build model\n",
    "\n",
    "# input shape is shape of one image, one single array\n",
    "print(X_train.shape[1:])\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# first layer \n",
    "model.add(MaxPooling2D(pool_size =(3, 3), padding='same', strides=(1, 1)))\n",
    "# 32 is output shape\n",
    "model.add(Activation('relu'))\n",
    "# Rectified Linear Activation. \n",
    "# This activation function has been proven to work well in neural networks.\n",
    "\n",
    "# second layer\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# training the model\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50, 1)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 791s 13ms/sample - loss: 1.1312 - accuracy: 0.6297 - val_loss: 0.6641 - val_accuracy: 0.8253\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 781s 13ms/sample - loss: 0.4906 - accuracy: 0.8663 - val_loss: 0.4180 - val_accuracy: 0.8851\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 828s 14ms/sample - loss: 0.3169 - accuracy: 0.9149 - val_loss: 0.2980 - val_accuracy: 0.9199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x215b6579d48>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Trying a deeper architecture\n",
    "# just add two more layers\n",
    "\n",
    "# build model\n",
    "\n",
    "# input shape is shape of one image, one single array\n",
    "print(X_train.shape[1:])\n",
    "\n",
    "model = Sequential()\n",
    "# Sequential is the easiest way to build a model in Keras. \n",
    "# It allows you to build a model layer by layer.\n",
    "\n",
    "# first layer \n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=X_train.shape[1:]))\n",
    "# 32 is output shape\n",
    "model.add(Activation('relu'))\n",
    "# Rectified Linear Activation. \n",
    "# This activation function has been proven to work well in neural networks.\n",
    "\n",
    "# second layer\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))   \n",
    "\n",
    "# training the model\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50, 1)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 555s 9ms/sample - loss: 1.5550 - accuracy: 0.4602 - val_loss: 1.1659 - val_accuracy: 0.6058\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 570s 9ms/sample - loss: 0.9938 - accuracy: 0.6877 - val_loss: 0.9057 - val_accuracy: 0.7232\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 618s 10ms/sample - loss: 0.8071 - accuracy: 0.7620 - val_loss: 0.7636 - val_accuracy: 0.7768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x215b7da1bc8>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropout on convolutional layers\n",
    "\n",
    "# deactivating certain individual nodes at random during training \n",
    "# we can simulate an ensemble of neural network with different architectures.\n",
    "\n",
    "# build model\n",
    "\n",
    "# input shape is shape of one image, one single array\n",
    "print(X_train.shape[1:])\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))   \n",
    "\n",
    "# training the model\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50, 1)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 488s 8ms/sample - loss: 1.1381 - accuracy: 0.6378 - val_loss: 0.7503 - val_accuracy: 0.7936\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 433s 7ms/sample - loss: 0.5734 - accuracy: 0.8399 - val_loss: 0.5442 - val_accuracy: 0.8604\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 462s 8ms/sample - loss: 0.4178 - accuracy: 0.8867 - val_loss: 0.4492 - val_accuracy: 0.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x215b6974dc8>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pooling, replace one layer, no activations fct. after MaxPooling\n",
    "\n",
    "# In \"pool\" of the 3x3 matrix of the convolution the highest value is selected\n",
    "# same network as before but add pooling\n",
    "\n",
    "# build model\n",
    "\n",
    "# input shape is shape of one image, one single array\n",
    "print(X_train.shape[1:])\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size =(3, 3), padding='same', strides=(1, 1)))\n",
    "# does max pooling need no activation function???? Does it have to be in combination woth the Conv2\n",
    "# or does the entire thing work just by using the MaxPooling2D fct\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# training the model\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml37",
   "language": "python",
   "name": "ml37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
