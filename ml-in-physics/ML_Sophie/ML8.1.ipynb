{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.polynomial.polynomial import polyval2d\n",
    "\n",
    "from sklearn.datasets import make_circles, make_moons, make_blobs \n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from tensorflow.keras import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 8\n",
    "### Sheet 7: Regular AutoEncoder \n",
    "- compare with VAE fom Sheet 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- 1-channel what meant by that?\n",
    "- Why was hidden_size = 2 chosen?\n",
    "- Any interpretation of the two different latent spaces achieved eith the deeper and 1 layer regular autoencoder? I don't really know how to interpret this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create imagesdef my_polys(degree):\n",
    "#     coeff = np.random.uniform(0,1,(degree+1,degree+1))\n",
    "#     # p = polyval2d(x, y, coef) # returns one value\n",
    "#     p = [[sum([coeff[i,j]*((x/size)**i)*((y/size)**j)\n",
    "#             for i in range(degree+1) for j in range(degree+1) if (i+j)<=degree]) \n",
    "#             for x in range(size)] for y in range(size)]\n",
    "#     return p\n",
    "\n",
    "# # one image\n",
    "# # x = np.linspace(0,1,40)\n",
    "# # y = np.linspace(0,1,40)\n",
    "# # x,y = np.meshgrid(x,y)\n",
    "# # image = my_polys(x,y,5)\n",
    "\n",
    "# maxdegree = 5\n",
    "# size = 40\n",
    "# num_polys = 1000 # number of \n",
    "# dataset = np.array([my_polys(np.random.randint(0,maxdegree)) for i in range(num_polys)])\n",
    "# dataset = tf.keras.utils.normalize(dataset)\n",
    "\n",
    "# numpy.polynomial.polynomial.polyval2d does what was written in forum on how to create images\n",
    "# Row numbers in V correspond to powers of x, while column numbers in V correspond to powers of y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 40, 40)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x274c1091348>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.shape(dataset))\n",
    "\n",
    "plt.imshow(dataset[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder is an unsupervised artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible.\n",
    "\n",
    "Autoencoder, by design, reduces data dimensions by learning how to ignore the noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 40\n",
    "\n",
    "# Evaluate polynomial over grid of size 40x40\n",
    "\n",
    "def polynomial(degree):\n",
    "    coeff = np.random.normal(0,1,(degree+1, degree+1))\n",
    "    #coeff = np.random.uniform(-1,1,(degree+1, degree+1))\n",
    "    return [[sum([coeff[i,j]*((x/size)**i)*((y/size)**j)\n",
    "            for i in range(degree+1) for j in range(degree+1) if (i+j)<=degree]) \n",
    "            for x in range(size)] for y in range(size)]\n",
    "\n",
    "# Degree two polynomials\n",
    "\n",
    "Npoly = 3000\n",
    "deg2polydata = np.array([polynomial(2) for i in range(Npoly)])\n",
    "deg2polydata = deg2polydata.reshape(Npoly, size*size)\n",
    "deg2mean = np.mean(deg2polydata)\n",
    "deg2sdev = np.std(deg2polydata)\n",
    "deg2polydata = tf.keras.utils.normalize(deg2polydata)\n",
    "np.save('deg2polydata.npy', deg2polydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder\n",
    "- Build two autoencoder architectures (e.g. similar to the ones presented in the lecture)\n",
    "- a single hidden dense layer and several hidden dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 454us/sample - loss: 0.0228\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 108us/sample - loss: 0.0161\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 68us/sample - loss: 0.0095\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.0050\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 77us/sample - loss: 0.0026\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 63us/sample - loss: 0.0014\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 113us/sample - loss: 9.7769e-04\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 63us/sample - loss: 7.9666e-04\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 94us/sample - loss: 7.2778e-04\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 72us/sample - loss: 7.0280e-04\n"
     ]
    }
   ],
   "source": [
    "# single hidden layer\n",
    "\n",
    "input_size = size**2\n",
    "hidden_size = 2 # latent dim \n",
    "output_size = size**2\n",
    "batch_size=100\n",
    "\n",
    "polydata = dataset.reshape(num_polys, size**2)\n",
    "\n",
    "inputs = Input(shape=(input_size,))\n",
    "\n",
    "# hidden layer\n",
    "AE1encoded = Dense(2, activation='linear')(inputs) # encoded\n",
    "\n",
    "AE1decoded = Dense(size**2, activation='linear')(AE1encoded) # decoded, lossy reconstruction of the input\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "AE1 = Model(inputs, AE1decoded)\n",
    "\n",
    "# Now let's train our autoencoder\n",
    "AE1.compile(optimizer='adam', loss='mse')\n",
    "AE1_initial_weights = AE1.get_weights()\n",
    "\n",
    "# train one layer encoder\n",
    "hist = AE1.fit(polydata, polydata, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "# model which computes the output of the hidden layer\n",
    "AE1_latent = Model(inputs, AE1encoded)\n",
    "\n",
    "# encode and decode some images\n",
    "AE1_encoded = AE1_latent.predict(polydata)\n",
    "\n",
    "# plot the latent dims\n",
    "# print(np.shape(AE1_encoded))\n",
    "print(AE1_encoded[:,0].shape)\n",
    "_ = plt.scatter(AE1_encoded[:,0], AE1_encoded[:,1],s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1600)\n",
      "Train on 1000 samples\n",
      "Epoch 1/200\n",
      "1000/1000 [==============================] - 1s 799us/sample - loss: 0.0128\n",
      "Epoch 2/200\n",
      "1000/1000 [==============================] - 0s 333us/sample - loss: 0.0016\n",
      "Epoch 3/200\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 9.2553e-04\n",
      "Epoch 4/200\n",
      "1000/1000 [==============================] - 0s 322us/sample - loss: 7.6822e-04\n",
      "Epoch 5/200\n",
      "1000/1000 [==============================] - 0s 344us/sample - loss: 6.9810e-04\n",
      "Epoch 6/200\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 6.3333e-04\n",
      "Epoch 7/200\n",
      "1000/1000 [==============================] - 0s 339us/sample - loss: 4.8754e-04\n",
      "Epoch 8/200\n",
      "1000/1000 [==============================] - 0s 342us/sample - loss: 2.3173e-04s - loss: 3.0419e\n",
      "Epoch 9/200\n",
      "1000/1000 [==============================] - 0s 344us/sample - loss: 7.9652e-05\n",
      "Epoch 10/200\n",
      "1000/1000 [==============================] - 0s 333us/sample - loss: 4.1154e-05\n",
      "Epoch 11/200\n",
      "1000/1000 [==============================] - 0s 350us/sample - loss: 3.5380e-05\n",
      "Epoch 12/200\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 3.2429e-05\n",
      "Epoch 13/200\n",
      "1000/1000 [==============================] - 0s 326us/sample - loss: 3.1179e-05\n",
      "Epoch 14/200\n",
      "1000/1000 [==============================] - 0s 363us/sample - loss: 3.0318e-05\n",
      "Epoch 15/200\n",
      "1000/1000 [==============================] - 0s 381us/sample - loss: 3.0060e-05\n",
      "Epoch 16/200\n",
      "1000/1000 [==============================] - 0s 334us/sample - loss: 3.0072e-05\n",
      "Epoch 17/200\n",
      "1000/1000 [==============================] - 0s 340us/sample - loss: 3.0920e-05\n",
      "Epoch 18/200\n",
      "1000/1000 [==============================] - 0s 357us/sample - loss: 3.0934e-05\n",
      "Epoch 19/200\n",
      "1000/1000 [==============================] - 0s 493us/sample - loss: 2.9187e-05\n",
      "Epoch 20/200\n",
      "1000/1000 [==============================] - 1s 634us/sample - loss: 2.8703e-05\n",
      "Epoch 21/200\n",
      "1000/1000 [==============================] - 0s 409us/sample - loss: 2.9676e-05\n",
      "Epoch 22/200\n",
      "1000/1000 [==============================] - 0s 362us/sample - loss: 3.7467e-05\n",
      "Epoch 23/200\n",
      "1000/1000 [==============================] - 0s 403us/sample - loss: 3.0376e-05\n",
      "Epoch 24/200\n",
      "1000/1000 [==============================] - 0s 343us/sample - loss: 2.8573e-05\n",
      "Epoch 25/200\n",
      "1000/1000 [==============================] - 0s 323us/sample - loss: 2.7779e-05\n",
      "Epoch 26/200\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 2.7569e-05\n",
      "Epoch 27/200\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 2.7236e-05\n",
      "Epoch 28/200\n",
      "1000/1000 [==============================] - 0s 333us/sample - loss: 2.7224e-05\n",
      "Epoch 29/200\n",
      "1000/1000 [==============================] - 0s 324us/sample - loss: 2.6830e-05\n",
      "Epoch 30/200\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 2.8220e-05\n",
      "Epoch 31/200\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 3.5614e-05\n",
      "Epoch 32/200\n",
      "1000/1000 [==============================] - 0s 443us/sample - loss: 2.8726e-05\n",
      "Epoch 33/200\n",
      "1000/1000 [==============================] - 0s 335us/sample - loss: 2.9161e-05\n",
      "Epoch 34/200\n",
      "1000/1000 [==============================] - 0s 384us/sample - loss: 3.7402e-05\n",
      "Epoch 35/200\n",
      "1000/1000 [==============================] - 0s 371us/sample - loss: 2.9240e-05\n",
      "Epoch 36/200\n",
      "1000/1000 [==============================] - 0s 356us/sample - loss: 2.6472e-05\n",
      "Epoch 37/200\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 2.5519e-05\n",
      "Epoch 38/200\n",
      "1000/1000 [==============================] - 0s 368us/sample - loss: 2.5239e-05\n",
      "Epoch 39/200\n",
      "1000/1000 [==============================] - 0s 367us/sample - loss: 2.6813e-05\n",
      "Epoch 40/200\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 5.9071e-05\n",
      "Epoch 41/200\n",
      "1000/1000 [==============================] - 0s 310us/sample - loss: 3.4556e-05\n",
      "Epoch 42/200\n",
      "1000/1000 [==============================] - 0s 327us/sample - loss: 2.7085e-05\n",
      "Epoch 43/200\n",
      "1000/1000 [==============================] - 0s 377us/sample - loss: 2.4987e-05\n",
      "Epoch 44/200\n",
      "1000/1000 [==============================] - 0s 388us/sample - loss: 2.4589e-05\n",
      "Epoch 45/200\n",
      "1000/1000 [==============================] - 0s 336us/sample - loss: 2.4628e-05\n",
      "Epoch 46/200\n",
      "1000/1000 [==============================] - 0s 356us/sample - loss: 2.4394e-05\n",
      "Epoch 47/200\n",
      "1000/1000 [==============================] - 0s 340us/sample - loss: 2.4157e-05\n",
      "Epoch 48/200\n",
      "1000/1000 [==============================] - 0s 325us/sample - loss: 2.4100e-05\n",
      "Epoch 49/200\n",
      "1000/1000 [==============================] - 0s 328us/sample - loss: 2.4130e-05\n",
      "Epoch 50/200\n",
      "1000/1000 [==============================] - 0s 341us/sample - loss: 2.4331e-05\n",
      "Epoch 51/200\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 2.4612e-05\n",
      "Epoch 52/200\n",
      "1000/1000 [==============================] - 0s 362us/sample - loss: 2.5072e-05\n",
      "Epoch 53/200\n",
      "1000/1000 [==============================] - 0s 365us/sample - loss: 2.5075e-05\n",
      "Epoch 54/200\n",
      "1000/1000 [==============================] - 0s 401us/sample - loss: 2.4210e-05\n",
      "Epoch 55/200\n",
      "1000/1000 [==============================] - 0s 377us/sample - loss: 2.4095e-05\n",
      "Epoch 56/200\n",
      "1000/1000 [==============================] - 0s 370us/sample - loss: 2.4808e-05\n",
      "Epoch 57/200\n",
      "1000/1000 [==============================] - 0s 402us/sample - loss: 2.5703e-05\n",
      "Epoch 58/200\n",
      "1000/1000 [==============================] - 0s 395us/sample - loss: 2.9247e-05\n",
      "Epoch 59/200\n",
      "1000/1000 [==============================] - 0s 373us/sample - loss: 2.9907e-05\n",
      "Epoch 60/200\n",
      "1000/1000 [==============================] - 0s 357us/sample - loss: 2.5743e-05\n",
      "Epoch 61/200\n",
      "1000/1000 [==============================] - 0s 417us/sample - loss: 2.4731e-05\n",
      "Epoch 62/200\n",
      "1000/1000 [==============================] - 0s 339us/sample - loss: 2.4313e-05\n",
      "Epoch 63/200\n",
      "1000/1000 [==============================] - 0s 359us/sample - loss: 2.8103e-05\n",
      "Epoch 64/200\n",
      "1000/1000 [==============================] - 0s 350us/sample - loss: 5.6193e-05\n",
      "Epoch 65/200\n",
      "1000/1000 [==============================] - 0s 353us/sample - loss: 3.6291e-05\n",
      "Epoch 66/200\n",
      "1000/1000 [==============================] - 0s 374us/sample - loss: 2.7328e-05\n",
      "Epoch 67/200\n",
      "1000/1000 [==============================] - 0s 346us/sample - loss: 2.5398e-05\n",
      "Epoch 68/200\n",
      "1000/1000 [==============================] - 0s 367us/sample - loss: 2.4517e-05\n",
      "Epoch 69/200\n",
      "1000/1000 [==============================] - 0s 330us/sample - loss: 2.4679e-05\n",
      "Epoch 70/200\n",
      "1000/1000 [==============================] - 0s 338us/sample - loss: 2.4481e-05\n",
      "Epoch 71/200\n",
      "1000/1000 [==============================] - 0s 367us/sample - loss: 2.4041e-05\n",
      "Epoch 72/200\n",
      "1000/1000 [==============================] - 0s 352us/sample - loss: 2.5040e-05\n",
      "Epoch 73/200\n",
      "1000/1000 [==============================] - 0s 332us/sample - loss: 2.4487e-05\n",
      "Epoch 74/200\n",
      "1000/1000 [==============================] - 0s 363us/sample - loss: 2.3998e-05\n",
      "Epoch 75/200\n",
      "1000/1000 [==============================] - 0s 350us/sample - loss: 2.4303e-05\n",
      "Epoch 76/200\n",
      "1000/1000 [==============================] - 0s 329us/sample - loss: 2.6918e-05\n",
      "Epoch 77/200\n",
      "1000/1000 [==============================] - 0s 360us/sample - loss: 3.0002e-05\n",
      "Epoch 78/200\n",
      "1000/1000 [==============================] - 0s 364us/sample - loss: 2.6356e-05\n",
      "Epoch 79/200\n",
      "1000/1000 [==============================] - 0s 367us/sample - loss: 2.5092e-05\n",
      "Epoch 80/200\n",
      "1000/1000 [==============================] - 0s 336us/sample - loss: 2.4435e-05\n",
      "Epoch 81/200\n",
      "1000/1000 [==============================] - 0s 345us/sample - loss: 2.5128e-05\n",
      "Epoch 82/200\n",
      "1000/1000 [==============================] - 0s 394us/sample - loss: 2.4631e-05\n",
      "Epoch 83/200\n",
      "1000/1000 [==============================] - 0s 326us/sample - loss: 2.4857e-05\n",
      "Epoch 84/200\n",
      "1000/1000 [==============================] - 0s 431us/sample - loss: 2.5256e-05\n",
      "Epoch 85/200\n",
      "1000/1000 [==============================] - 0s 367us/sample - loss: 2.4865e-05\n",
      "Epoch 86/200\n",
      "1000/1000 [==============================] - 0s 376us/sample - loss: 2.4554e-05\n",
      "Epoch 87/200\n",
      "1000/1000 [==============================] - 0s 437us/sample - loss: 2.4379e-05\n",
      "Epoch 88/200\n",
      "1000/1000 [==============================] - 0s 390us/sample - loss: 2.5494e-05\n",
      "Epoch 89/200\n",
      "1000/1000 [==============================] - 0s 355us/sample - loss: 2.4783e-05\n",
      "Epoch 90/200\n",
      "1000/1000 [==============================] - 0s 386us/sample - loss: 2.4234e-05\n",
      "Epoch 91/200\n",
      "1000/1000 [==============================] - 0s 323us/sample - loss: 2.5658e-05\n",
      "Epoch 92/200\n",
      "1000/1000 [==============================] - 0s 324us/sample - loss: 6.2965e-05\n",
      "Epoch 93/200\n",
      "1000/1000 [==============================] - 0s 339us/sample - loss: 3.9516e-05\n",
      "Epoch 94/200\n",
      "1000/1000 [==============================] - 0s 384us/sample - loss: 2.8603e-05\n",
      "Epoch 95/200\n",
      "1000/1000 [==============================] - 0s 356us/sample - loss: 2.5464e-05\n",
      "Epoch 96/200\n",
      "1000/1000 [==============================] - 0s 388us/sample - loss: 2.4940e-05\n",
      "Epoch 97/200\n",
      "1000/1000 [==============================] - 0s 328us/sample - loss: 2.4651e-05\n",
      "Epoch 98/200\n",
      "1000/1000 [==============================] - 0s 350us/sample - loss: 2.7822e-05\n",
      "Epoch 99/200\n",
      "1000/1000 [==============================] - 0s 348us/sample - loss: 2.5162e-05\n",
      "Epoch 100/200\n",
      "1000/1000 [==============================] - 0s 348us/sample - loss: 2.4050e-05\n",
      "Epoch 101/200\n",
      "1000/1000 [==============================] - 0s 366us/sample - loss: 2.3884e-05\n",
      "Epoch 102/200\n",
      "1000/1000 [==============================] - 0s 404us/sample - loss: 2.5260e-05\n",
      "Epoch 103/200\n",
      "1000/1000 [==============================] - 0s 342us/sample - loss: 2.6296e-05\n",
      "Epoch 104/200\n",
      "1000/1000 [==============================] - 0s 325us/sample - loss: 2.5831e-05\n",
      "Epoch 105/200\n",
      "1000/1000 [==============================] - 0s 340us/sample - loss: 2.6728e-05\n",
      "Epoch 106/200\n",
      "1000/1000 [==============================] - 0s 451us/sample - loss: 2.5443e-05\n",
      "Epoch 107/200\n",
      "1000/1000 [==============================] - 0s 381us/sample - loss: 2.5897e-05\n",
      "Epoch 108/200\n",
      "1000/1000 [==============================] - 1s 563us/sample - loss: 2.5860e-05\n",
      "Epoch 109/200\n",
      "1000/1000 [==============================] - 1s 631us/sample - loss: 2.5052e-05\n",
      "Epoch 110/200\n",
      "1000/1000 [==============================] - 1s 500us/sample - loss: 2.4771e-05\n",
      "Epoch 111/200\n",
      "1000/1000 [==============================] - 0s 455us/sample - loss: 2.4290e-05\n",
      "Epoch 112/200\n",
      "1000/1000 [==============================] - 0s 329us/sample - loss: 2.4421e-05\n",
      "Epoch 113/200\n",
      "1000/1000 [==============================] - 0s 348us/sample - loss: 2.4524e-05\n",
      "Epoch 114/200\n",
      "1000/1000 [==============================] - 0s 353us/sample - loss: 2.4088e-05\n",
      "Epoch 115/200\n",
      "1000/1000 [==============================] - 0s 372us/sample - loss: 2.5524e-05\n",
      "Epoch 116/200\n",
      "1000/1000 [==============================] - 0s 415us/sample - loss: 3.0229e-05\n",
      "Epoch 117/200\n",
      "1000/1000 [==============================] - 0s 344us/sample - loss: 2.8051e-05\n",
      "Epoch 118/200\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 2.6628e-05\n",
      "Epoch 119/200\n",
      "1000/1000 [==============================] - 0s 405us/sample - loss: 2.4643e-05\n",
      "Epoch 120/200\n",
      "1000/1000 [==============================] - 0s 415us/sample - loss: 2.4424e-05\n",
      "Epoch 121/200\n",
      "1000/1000 [==============================] - 0s 428us/sample - loss: 2.4184e-05\n",
      "Epoch 122/200\n",
      "1000/1000 [==============================] - 0s 399us/sample - loss: 2.3878e-05\n",
      "Epoch 123/200\n",
      "1000/1000 [==============================] - 0s 350us/sample - loss: 2.3824e-05\n",
      "Epoch 124/200\n",
      "1000/1000 [==============================] - 0s 396us/sample - loss: 2.3831e-05\n",
      "Epoch 125/200\n",
      "1000/1000 [==============================] - 0s 370us/sample - loss: 2.3954e-05\n",
      "Epoch 126/200\n",
      "1000/1000 [==============================] - 0s 379us/sample - loss: 2.4350e-05\n",
      "Epoch 127/200\n",
      "1000/1000 [==============================] - 0s 332us/sample - loss: 2.6293e-05\n",
      "Epoch 128/200\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 3.4894e-05\n",
      "Epoch 129/200\n",
      "1000/1000 [==============================] - 0s 372us/sample - loss: 2.5200e-05s - loss: 2.4328e\n",
      "Epoch 130/200\n",
      "1000/1000 [==============================] - 0s 370us/sample - loss: 2.4086e-05\n",
      "Epoch 131/200\n",
      "1000/1000 [==============================] - 0s 383us/sample - loss: 2.3895e-05\n",
      "Epoch 132/200\n",
      "1000/1000 [==============================] - 0s 379us/sample - loss: 2.3942e-05s - loss: 2.1984\n",
      "Epoch 133/200\n",
      "1000/1000 [==============================] - 0s 349us/sample - loss: 2.3767e-05\n",
      "Epoch 134/200\n",
      "1000/1000 [==============================] - 0s 374us/sample - loss: 2.3907e-05\n",
      "Epoch 135/200\n",
      "1000/1000 [==============================] - 0s 377us/sample - loss: 2.3814e-05\n",
      "Epoch 136/200\n",
      "1000/1000 [==============================] - 0s 422us/sample - loss: 2.3847e-05\n",
      "Epoch 137/200\n",
      "1000/1000 [==============================] - 0s 350us/sample - loss: 2.5421e-05\n",
      "Epoch 138/200\n",
      "1000/1000 [==============================] - 1s 519us/sample - loss: 4.3544e-05\n",
      "Epoch 139/200\n",
      "1000/1000 [==============================] - 0s 421us/sample - loss: 2.7308e-05\n",
      "Epoch 140/200\n",
      "1000/1000 [==============================] - 0s 347us/sample - loss: 2.5105e-05\n",
      "Epoch 141/200\n",
      "1000/1000 [==============================] - 0s 431us/sample - loss: 2.5297e-05\n",
      "Epoch 142/200\n",
      "1000/1000 [==============================] - 0s 482us/sample - loss: 2.4975e-05\n",
      "Epoch 143/200\n",
      "1000/1000 [==============================] - 1s 506us/sample - loss: 2.3938e-05\n",
      "Epoch 144/200\n",
      "1000/1000 [==============================] - 1s 500us/sample - loss: 2.4269e-05\n",
      "Epoch 145/200\n",
      "1000/1000 [==============================] - 0s 418us/sample - loss: 2.4022e-05\n",
      "Epoch 146/200\n",
      "1000/1000 [==============================] - 0s 364us/sample - loss: 2.4820e-05\n",
      "Epoch 147/200\n",
      "1000/1000 [==============================] - 0s 341us/sample - loss: 2.4596e-05\n",
      "Epoch 148/200\n",
      "1000/1000 [==============================] - 0s 330us/sample - loss: 2.4610e-05\n",
      "Epoch 149/200\n",
      "1000/1000 [==============================] - 0s 364us/sample - loss: 2.4337e-05\n",
      "Epoch 150/200\n",
      "1000/1000 [==============================] - 0s 310us/sample - loss: 2.4391e-05\n",
      "Epoch 151/200\n",
      "1000/1000 [==============================] - 0s 360us/sample - loss: 2.4074e-05\n",
      "Epoch 152/200\n",
      "1000/1000 [==============================] - 0s 340us/sample - loss: 2.7198e-05\n",
      "Epoch 153/200\n",
      "1000/1000 [==============================] - 0s 331us/sample - loss: 3.3525e-05\n",
      "Epoch 154/200\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 2.6296e-05\n",
      "Epoch 155/200\n",
      "1000/1000 [==============================] - 0s 372us/sample - loss: 2.5216e-05\n",
      "Epoch 156/200\n",
      "1000/1000 [==============================] - 0s 413us/sample - loss: 2.7921e-05\n",
      "Epoch 157/200\n",
      "1000/1000 [==============================] - 0s 322us/sample - loss: 3.0195e-05\n",
      "Epoch 158/200\n",
      "1000/1000 [==============================] - 0s 329us/sample - loss: 2.7439e-05\n",
      "Epoch 159/200\n",
      "1000/1000 [==============================] - 0s 399us/sample - loss: 2.4782e-05\n",
      "Epoch 160/200\n",
      "1000/1000 [==============================] - 0s 343us/sample - loss: 2.4333e-05\n",
      "Epoch 161/200\n",
      "1000/1000 [==============================] - 0s 322us/sample - loss: 2.4468e-05\n",
      "Epoch 162/200\n",
      "1000/1000 [==============================] - 0s 296us/sample - loss: 2.4100e-05\n",
      "Epoch 163/200\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 2.4460e-05\n",
      "Epoch 164/200\n",
      "1000/1000 [==============================] - 0s 337us/sample - loss: 2.4368e-05\n",
      "Epoch 165/200\n",
      "1000/1000 [==============================] - 0s 324us/sample - loss: 2.4953e-05\n",
      "Epoch 166/200\n",
      "1000/1000 [==============================] - 0s 329us/sample - loss: 5.4718e-05\n",
      "Epoch 167/200\n",
      "1000/1000 [==============================] - 0s 334us/sample - loss: 3.4029e-05\n",
      "Epoch 168/200\n",
      "1000/1000 [==============================] - 0s 324us/sample - loss: 2.7999e-05\n",
      "Epoch 169/200\n",
      "1000/1000 [==============================] - 0s 308us/sample - loss: 2.5363e-05\n",
      "Epoch 170/200\n",
      "1000/1000 [==============================] - 0s 308us/sample - loss: 2.4406e-05\n",
      "Epoch 171/200\n",
      "1000/1000 [==============================] - 0s 328us/sample - loss: 2.3841e-05\n",
      "Epoch 172/200\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 2.4116e-05\n",
      "Epoch 173/200\n",
      "1000/1000 [==============================] - 0s 296us/sample - loss: 2.3896e-05\n",
      "Epoch 174/200\n",
      "1000/1000 [==============================] - 0s 365us/sample - loss: 2.3992e-05\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 358us/sample - loss: 2.4363e-05\n",
      "Epoch 176/200\n",
      "1000/1000 [==============================] - 0s 333us/sample - loss: 2.4261e-05\n",
      "Epoch 177/200\n",
      "1000/1000 [==============================] - 0s 331us/sample - loss: 2.3921e-05\n",
      "Epoch 178/200\n",
      "1000/1000 [==============================] - 0s 330us/sample - loss: 2.3942e-05\n",
      "Epoch 179/200\n",
      "1000/1000 [==============================] - 0s 343us/sample - loss: 2.4373e-05\n",
      "Epoch 180/200\n",
      "1000/1000 [==============================] - 0s 352us/sample - loss: 2.4448e-05\n",
      "Epoch 181/200\n",
      "1000/1000 [==============================] - 0s 322us/sample - loss: 2.4015e-05\n",
      "Epoch 182/200\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 2.4480e-05\n",
      "Epoch 183/200\n",
      "1000/1000 [==============================] - 0s 376us/sample - loss: 2.4069e-05\n",
      "Epoch 184/200\n",
      "1000/1000 [==============================] - 0s 354us/sample - loss: 2.3836e-05\n",
      "Epoch 185/200\n",
      "1000/1000 [==============================] - 0s 412us/sample - loss: 2.4234e-05\n",
      "Epoch 186/200\n",
      "1000/1000 [==============================] - 0s 428us/sample - loss: 2.4480e-05\n",
      "Epoch 187/200\n",
      "1000/1000 [==============================] - 0s 336us/sample - loss: 2.4286e-05\n",
      "Epoch 188/200\n",
      "1000/1000 [==============================] - 0s 331us/sample - loss: 2.4943e-05\n",
      "Epoch 189/200\n",
      "1000/1000 [==============================] - 0s 406us/sample - loss: 2.4203e-05\n",
      "Epoch 190/200\n",
      "1000/1000 [==============================] - 0s 388us/sample - loss: 2.5346e-05\n",
      "Epoch 191/200\n",
      "1000/1000 [==============================] - 0s 352us/sample - loss: 2.4683e-05\n",
      "Epoch 192/200\n",
      "1000/1000 [==============================] - 0s 386us/sample - loss: 2.4517e-05\n",
      "Epoch 193/200\n",
      "1000/1000 [==============================] - 0s 448us/sample - loss: 3.6836e-05\n",
      "Epoch 194/200\n",
      "1000/1000 [==============================] - 0s 375us/sample - loss: 4.1949e-05\n",
      "Epoch 195/200\n",
      "1000/1000 [==============================] - 0s 334us/sample - loss: 2.8423e-05\n",
      "Epoch 196/200\n",
      "1000/1000 [==============================] - 0s 421us/sample - loss: 2.6877e-05\n",
      "Epoch 197/200\n",
      "1000/1000 [==============================] - 0s 311us/sample - loss: 2.5615e-05\n",
      "Epoch 198/200\n",
      "1000/1000 [==============================] - 0s 357us/sample - loss: 2.5262e-05\n",
      "Epoch 199/200\n",
      "1000/1000 [==============================] - 0s 348us/sample - loss: 2.4962e-05\n",
      "Epoch 200/200\n",
      "1000/1000 [==============================] - 0s 327us/sample - loss: 2.4516e-05\n"
     ]
    }
   ],
   "source": [
    "# several hidden dense layers\n",
    "\n",
    "intermediate_dim1 = 512\n",
    "intermedite_dim2 = 64\n",
    "latent_dim = 2\n",
    "\n",
    "AE2compress1 = Dense(512, activation='relu')(inputs)\n",
    "AE2compress2 = Dense(64, activation='relu')(AE2compress1)\n",
    "\n",
    "AE2encoded = Dense(2, activation='linear')(AE2compress2)\n",
    "\n",
    "\n",
    "AE2decompress1 = Dense(64, activation='relu')(AE2encoded)\n",
    "AE2decompress2 = Dense(512, activation='relu')(AE2decompress1)\n",
    "AE2decoded = Dense(size**2, activation='linear')(AE2decompress2)\n",
    "\n",
    "AE2 = Model(inputs, AE2decoded)\n",
    "AE2.compile(optimizer='adam', loss='mse')\n",
    "AE2_initial_weights = AE2.get_weights()\n",
    "\n",
    "print(np.shape(polydata))\n",
    "\n",
    "# compile and train the larger autoencoder\n",
    "hist_large = AE2.fit(polydata, polydata, epochs=200, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x274bfe483c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# more complex autoencoder\n",
    "\n",
    "# model which computes the output of the hidden layer\n",
    "AE2_latent = Model(inputs, AE2encoded)\n",
    "\n",
    "# plot latent dimensions\n",
    "AE2_encoded = AE2_latent.predict(polydata)\n",
    "AE2_encoded[:,0].shape\n",
    "plt.scatter(AE2_encoded[:,0], AE2_encoded[:,1],s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 8 \n",
    "I am just copy pasting some info which I found helpful, so I can access it later. I left it in here because I thought I could be interesting. \n",
    "### VAE\n",
    "- VAE is an autoencoder whose encodings distribution is regularised during the training in order to ensure that its latent space has good properties allowing us to generate some new data\n",
    "\n",
    "### PCA\n",
    "- idea of PCA is to build n_e new independent features that are linear combinations of the n_d old features\n",
    "- best linear subspace of the initial space\n",
    "\n",
    "### Regular autoencoder \n",
    "- difference to PCA: Moreover, for linear autoencoders and contrarily to PCA, the new features we end up do not have to be independent (no orthogonality constraints in the neural networks). PCA uses linear transformation whereas autoencoders use non-linear transformations.\n",
    "- once the autoencoder has been trained, we have both an encoder and a decoder but still no real way to produce any new content.\n",
    "- we could take a point randomly from that latent space and decode it to get a new content. regularity of the latent space for autoencoders is a difficult point that depends on the distribution of the data in the initial space, the dimension of the latent space and the architecture of the encoder. So, it is pretty difficult (if not impossible) to ensure, a priori, that the encoder will organize the latent space in a smart way compatible with the generative process we just described.\n",
    "- Possible issue: severe overfitting implying that some points of the latent space will give meaningless content once decoded.\n",
    "- ndeed, nothing in the task the autoencoder is trained for enforce to get such organisation: the autoencoder is solely trained to encode and decode with as few loss as possible, no matter how the latent space is organised.\n",
    "- unless we explicitly regularise it, natural that, during the training, the network takes advantage of any overfitting possibilities to achieve its task as well as it can\n",
    "\n",
    "### VAE\n",
    "- to be able to use the decoder of our autoencoder for generative purpose, we have to be sure that the latent space is regular enough. \n",
    "- training is regularised to avoid overfitting\n",
    "- instead of encoding an input as a single point, we encode it as a distribution over the latent space\n",
    "- The model is then trained as follows:\n",
    "    - first, the input is encoded as distribution over the latent space\n",
    "    - second, a point from the latent space is sampled from that distribution\n",
    "    - third, the sampled point is decoded and the reconstruction error can be computed\n",
    "    - finally, the reconstruction error is backpropagated through the network\n",
    "    \n",
    "- tends to regularise the organisation of the latent space by making the distributions returned by the encoder close to a standard normal distribution.\n",
    "- regularisation term is expressed as the Kulback-Leibler divergence between the returned distribution and a standard Gaussian \n",
    "\n",
    "- we have to regularise both the covariance matrix and the mean of the distributions returned by the encoder -> No \"points\", very tight gaussians; --> no spaced out means\n",
    "-  this regularisation is done by enforcing distributions to be close to a standard normal distribution (centred and reduced);  mean to be close to 0\n",
    "\n",
    "- The parameters of the model are trained via two loss functions: a reconstruction loss forcing the decoded samples to match the initial inputs (just like in our previous autoencoders), and the KL divergence between the learned latent distribution and the prior distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1600)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 512)          819712      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 2)            1026        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (100, 2)             0           dense_9[0][0]                    \n",
      "                                                                 dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 820,738\n",
      "Trainable params: 820,738\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1600)              820800    \n",
      "=================================================================\n",
      "Total params: 822,336\n",
      "Trainable params: 822,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1600)]            0         \n",
      "_________________________________________________________________\n",
      "model_4 (Model)              [(None, 2), (None, 2), (1 820738    \n",
      "_________________________________________________________________\n",
      "model_5 (Model)              multiple                  822336    \n",
      "=================================================================\n",
      "Total params: 1,643,074\n",
      "Trainable params: 1,643,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# tf.executing_eagerly()\n",
    "\n",
    "# inputs = Input(shape=(input_size,))\n",
    "# hidden_size = 2 # latent dim \n",
    "# batch_size=100\n",
    "\n",
    "intermediate_dim = 512\n",
    "latent_dim = 2\n",
    "input_size = size**2\n",
    "original_dim = size**2\n",
    "\n",
    "AE2compress1_vae = Dense(512, activation='relu')(inputs)\n",
    "AE2encoded_vae = Dense(2, activation='linear')(AE2compress1_vae)\n",
    "z_mean = AE2encoded_vae\n",
    "z_log_sigma =  AE2encoded_vae\n",
    "\n",
    "# sample a point similar to a point in latent space\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1.0)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "z = Lambda(sampling)([z_mean, z_log_sigma])\n",
    "\n",
    "# encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_sigma, z])\n",
    "encoder.summary()\n",
    "\n",
    "# build decoder model \n",
    "latent_inputs = Input(shape=(latent_dim,))\n",
    "AE2decompress2_vae = Dense(512, activation='relu')(latent_inputs)\n",
    "AE2decoded_vae = Dense(size**2, activation='linear')(AE2decompress2_vae)\n",
    "outputs = AE2decoded_vae\n",
    "\n",
    "# decoder model \n",
    "decoder = Model(latent_inputs, outputs) \n",
    "decoder.summary()\n",
    "\n",
    "# we need 3 models: third one is end-to-end autoencoder\n",
    "outputs = decoder(encoder(inputs)[2]) # what does this do exactly, why necessary\n",
    "vae = Model(inputs, outputs)\n",
    "\n",
    "# loss function: the sum of a reconstruction term, and the KL divergence regularization term\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "# reconstruction_loss = binary_crossentropy(inputs,outputs)\n",
    "# reconstruction_loss *= original_dim\n",
    "# kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)\n",
    "# kl_loss = K.sum(kl_loss, axis=-1)\n",
    "# kl_loss *= -0.5\n",
    "# vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "# vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "vae.compile(optimizer='adam', loss=vae_loss)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1600)\n",
      "Train on 1000 samples\n",
      "Epoch 1/4\n",
      " 100/1000 [==>...........................] - ETA: 4s"
     ]
    },
    {
     "ename": "_SymbolicException",
     "evalue": "Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'dense_9/Identity:0' shape=(None, 2) dtype=float32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: dense_9/Identity:0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31m_SymbolicException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6ab49f636510>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolydata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mhist_vae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolydata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolydata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     73\u001b[0m       raise core._SymbolicException(\n\u001b[0;32m     74\u001b[0m           \u001b[1;34m\"Inputs to eager execution function cannot be Keras symbolic \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m           \"tensors, but found {}\".format(keras_symbolic_tensors))\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m_SymbolicException\u001b[0m: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'dense_9/Identity:0' shape=(None, 2) dtype=float32>]"
     ]
    }
   ],
   "source": [
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "print(np.shape(polydata))\n",
    "\n",
    "hist_vae = vae.fit(polydata, polydata, epochs=4, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I keep getting an error when fitting vae I can't really compare the performance. \n",
    "But in the latent space of the VAE the point should be close together somewhat organised. The further away the points are the less similar the decoded output is. This was not e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot latent dimensions\n",
    "l = encoder.predict(polydata)\n",
    "# AE2_encoded_vae[:,0].shape\n",
    "# plt.scatter(AE2_encoded_vae[:,0], AE2_encoded_vae[:,1],s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml37",
   "language": "python",
   "name": "ml37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
