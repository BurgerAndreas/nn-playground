{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the last exercise sheet we have seen that a single layer Perceptron cannot accommodate the XOR function. Show that by utilising a single hidden layer with sigmoid activation functions, XOR can be realised.  \n",
    "* Implement this network utilising Keras.\n",
    "* What is the minimum number of hidden units needed in this network?\n",
    "* What are the network parameters after training?\n",
    "* $\\star$ Could there be significantly different results depending on the weight initialisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Keras Documentation for [Sequential model API](https://keras.io/models/sequential/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve it \"Regression Style\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR\n",
    "data = np.array([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
    "labels = np.array([[0.],[1.],[1.],[0.]])\n",
    "\n",
    "# We construct the neural network\n",
    "model = tf.keras.models.Sequential()\n",
    "# Input layer. Note that 2 is the dimensionality of the OUTPUT, i.e. the hidden layer\n",
    "model.add(tf.keras.layers.Dense(2, input_dim=2, activation='sigmoid'))\n",
    "# Hidden layer\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['binary_accuracy'])\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01), metrics=['binary_accuracy'])\n",
    "\n",
    "# Visualize the model\n",
    "tf.keras.utils.plot_model(model, 'model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Our Model](model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NN:\n",
    "model.fit(data, labels, epochs=6000,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that two hidden units are enough to learn XOR. If we only use one hidden unit, this basically reduces to the perceptron and one can see that the accuracy will not go above 0.75, which basically corresponds to learing OR instead of XOR. So two is indeed minimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code there is a commented out line where the loss function is replaced by the cross-entropy. This does not work reliably with a single output. The reason is that the cross-entropy is comparing probability distributions and with one output there is no probability interpretation. The cross-entropy is more suitable if we frame this as a classification problem and assign each class label an own output, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters after training:\n",
    "\n",
    "print('First layer weights:\\n',model.layers[0].get_weights()[0])\n",
    "print('First layer bias:\\n',model.layers[0].get_weights()[1])\n",
    "\n",
    "print('Second layer weights:\\n',model.layers[1].get_weights()[0])\n",
    "print('Second layer bias:\\n',model.layers[1].get_weights()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, the weights are initialized using small random numbers. If we use zeros instead, the NN is stuck and will never converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We construct the neural network\n",
    "model = tf.keras.Sequential()\n",
    "# Input layer. Note that 2 is the dimensionality of the OUTPUT, i.e. the hidden layer\n",
    "model.add(tf.keras.layers.Dense(2, input_dim=2, activation='sigmoid', kernel_initializer='zeros'))\n",
    "# Hidden layer\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer='zeros'))\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.01), metrics=['binary_accuracy'])\n",
    "\n",
    "# Train NN:\n",
    "model.fit(data, labels, epochs=6000,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be fairly obvious, but if we use crazy values for the weight initialization, it also stops working. This is because the sigmoid is very flat (vanishing gradient) for large values of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize to stupid values\n",
    "crazyInit = tf.keras.initializers.Constant(value=1e3)\n",
    "\n",
    "# We construct the neural network\n",
    "model = tf.keras.Sequential()\n",
    "# Input layer. Note that 2 is the dimensionality of the OUTPUT, i.e. the hidden layer\n",
    "model.add(tf.keras.layers.Dense(2, input_dim=2, activation='sigmoid', kernel_initializer=crazyInit))\n",
    "# Hidden layer\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=crazyInit))\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.01), metrics=['binary_accuracy'])\n",
    "\n",
    "# Train NN:\n",
    "model.fit(data, labels, epochs=6000,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve it \"Classification Style\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR\n",
    "data = np.array([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
    "labels = np.array([[0.,1.],[1.,0.],[1.,0.],[0.,1.]]) \n",
    "\n",
    "# We construct the neural network\n",
    "model = tf.keras.models.Sequential()\n",
    "# Input layer. Note that 2 is the dimensionality of the OUTPUT, i.e. the hidden layer\n",
    "model.add(tf.keras.layers.Dense(2, input_dim=2, activation='sigmoid'))\n",
    "# Hidden layer\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['binary_accuracy'])\n",
    "\n",
    "# train\n",
    "model.fit(data, labels, epochs=6000,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml37",
   "language": "python",
   "name": "ml37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
