{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variational autoencoder\n",
    "# with custom loss (KL)\n",
    "\n",
    "# https://towardsdatascience.com/variational-autoencoders-as-generative-models-with-keras-e0c79415a7eb\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, MaxPooling2D, Conv2D, Flatten, \\\n",
    "    Dense, Lambda, Reshape, UpSampling2D, Conv2DTranspose\n",
    "from tensorflow import shape, exp, reduce_mean, square\n",
    "from tensorflow.keras.backend import random_normal\n",
    "from tensorflow.keras import Model, losses\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# download mnist\n",
    "(trainX, trainy), (testX, testy) = mnist.load_data()\n",
    "\n",
    "print('Training data shapes: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
    "print('Testing data shapes: X=%s, y=%s' % (testX.shape, testy.shape))\n",
    "\n",
    "\"\"\"\n",
    "# print random pics\n",
    "for j in range(5):\n",
    "    i = np.random.randint(0, 10000)\n",
    "    plt.subplot(550 + 1 + j)\n",
    "    plt.imshow(trainX[i], cmap='gray')\n",
    "    plt.title(trainy[i])\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# preprocessing\n",
    "# normalize pixel values from (0,255) to (0,1)\n",
    "train_data = trainX.astype('float32')/255\n",
    "test_data = testX.astype('float32')/255\n",
    "# add dimension for image channels for Conv2D\n",
    "train_data = np.reshape(train_data, (60000, 28, 28, 1))\n",
    "test_data = np.reshape(test_data, (10000, 28, 28, 1))\n",
    "print (train_data.shape, test_data.shape)\n",
    "\n",
    "\n",
    "# encoder\n",
    "input_data = Input(shape=(28, 28, 1)) #tensorflow.keras.layers.Input\n",
    "# 64 output filters, 5x5 kernel size\n",
    "encoder = Conv2D(64, (5,5), activation='relu')(input_data)\n",
    "encoder = MaxPooling2D((2,2))(encoder)\n",
    "encoder = Conv2D(64, (3,3), activation='relu')(encoder)\n",
    "encoder = MaxPooling2D((2,2))(encoder)\n",
    "encoder = Conv2D(32, (3,3), activation='relu')(encoder)\n",
    "encoder = MaxPooling2D((2,2))(encoder)\n",
    "encoder = Flatten()(encoder)\n",
    "encoder = Dense(16)(encoder)\n",
    "\n",
    "# encoder latent features\n",
    "def sample_latent_features(distribution):\n",
    "    distribution_mean, distribution_variance = distribution\n",
    "    batch_size = shape(distribution_variance)[0]\n",
    "    random = random_normal(shape=(batch_size, shape(distribution_variance)[1]))\n",
    "    return distribution_mean + exp(0.5 * distribution_variance) * random\n",
    "\n",
    "distribution_mean = Dense(2, name='mean')(encoder)\n",
    "distribution_variance = Dense(2, name='log_variance')(encoder)\n",
    "latent_encoding = Lambda(sample_latent_features)([distribution_mean, distribution_variance])\n",
    "\n",
    "# build encoder\n",
    "encoder_model = Model(input_data, latent_encoding)\n",
    "encoder_model.summary()\n",
    "\n",
    "\n",
    "################\n",
    "# decoder\n",
    "decoder_input = Input(shape=(2))\n",
    "decoder = Dense(64)(decoder_input)\n",
    "decoder = Reshape((1, 1, 64))(decoder)\n",
    "decoder = Conv2DTranspose(64, (3,3), activation='relu')(decoder)\n",
    "\n",
    "decoder = Conv2DTranspose(64, (3,3), activation='relu')(decoder)\n",
    "decoder = UpSampling2D((2,2))(decoder)\n",
    "\n",
    "decoder = Conv2DTranspose(64, (3,3), activation='relu')(decoder)\n",
    "decoder = UpSampling2D((2,2))(decoder)\n",
    "\n",
    "decoder_output = Conv2DTranspose(1, (5,5), activation='relu')(decoder)\n",
    "\n",
    "decoder_model = Model(decoder_input, decoder_output)\n",
    "decoder_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# build VAE\n",
    "encoded = encoder_model(input_data)\n",
    "decoded = decoder_model(encoded)\n",
    "autoencoder = Model(input_data, decoded)\n",
    "autoencoder.summary()\n",
    "\n",
    "\n",
    "# custom loss (Reconstruction Loss + KL loss)\n",
    "def get_loss(distribution_mean, distribution_variance):\n",
    "    def get_reconstruction_loss(y_true, y_pred):\n",
    "        reconstruction_loss = losses.mse(y_true, y_pred)\n",
    "        reconstruction_loss_batch = reduce_mean(reconstruction_loss)\n",
    "        return reconstruction_loss_batch*28*28\n",
    "    def get_kl_loss(distribution_mean, distribution_variance):\n",
    "        kl_loss = 1 + distribution_variance - square(distribution_mean) - exp(distribution_variance)\n",
    "        kl_loss_batch = reduce_mean(kl_loss)\n",
    "        return kl_loss_batch\n",
    "    def total_loss(y_true, y_pred):\n",
    "        reconstruction_loss_batch = get_reconstruction_loss(y_true,y_pred)\n",
    "        kl_loss_batch = get_kl_loss(distribution_mean, distribution_variance)\n",
    "        return reconstruction_loss_batch + kl_loss_batch\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# compile\n",
    "autoencoder.compile(loss=get_loss(distribution_mean, distribution_variance), optimizer=\"adam\")\n",
    "\n",
    "# train\n",
    "autoencoder.fit(train_data, train_data, epochs=20, batch_size=64, validation_data=(test_data, test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-c4f24b27",
   "language": "python",
   "display_name": "PyCharm (MLinPhysics)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}