{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VAE with tensorflow probability\n",
    "\n",
    "# https://www.tensorflow.org/probability/examples/Probabilistic_Layers_VAE\n",
    "# tensorflow probability package\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# check for cpu\n",
    "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
    "  print('WARNING: GPU device not found.')\n",
    "else:\n",
    "  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "\n",
    "# load mnist dataset\n",
    "datasets, datasets_info = tfds.load(name='mnist',\n",
    "                                    with_info=True,\n",
    "                                    as_supervised=False)\n",
    "\n",
    "def _preprocess(sample):\n",
    "  image = tf.cast(sample['image'], tf.float32) / 255.  # Scale to unit interval.\n",
    "  image = image < tf.random.uniform(tf.shape(image))   # Randomly binarize.\n",
    "  return image, image\n",
    "\n",
    "train_dataset = (datasets['train']\n",
    "                 .map(_preprocess)\n",
    "                 .batch(256)\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "                 .shuffle(int(10e3)))\n",
    "eval_dataset = (datasets['test']\n",
    "                .map(_preprocess)\n",
    "                .batch(256)\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "\n",
    "# specify model\n",
    "input_shape = datasets_info.features['image'].shape\n",
    "# latent variable z, 16 dims\n",
    "encoded_size = 16\n",
    "base_depth = 32\n",
    "\n",
    "# prior, independent gaussian\n",
    "# no learned parameters\n",
    "tfd = tfp.distributions\n",
    "encoded_size = 16\n",
    "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                        reinterpreted_batch_ndims=1)\n",
    "\n",
    "\n",
    "# encoder\n",
    "encoder = tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=input_shape),\n",
    "    tfkl.Lambda(lambda x: tf.cast(x, tf.float32) - 0.5),\n",
    "    tfkl.Conv2D(base_depth, 5, strides=1,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2D(base_depth, 5, strides=2,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2D(2 * base_depth, 5, strides=1,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2D(2 * base_depth, 5, strides=2,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2D(4 * encoded_size, 7, strides=1,\n",
    "                padding='valid', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Flatten(),\n",
    "    tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "               activation=None),\n",
    "    # output of the encoder, a tfp layer\n",
    "    # already includes Kl loss term\n",
    "    tfpl.MultivariateNormalTriL(\n",
    "        encoded_size,\n",
    "        activity_regularizer=tfpl.KLDivergenceRegularizer(prior)),\n",
    "])\n",
    "\n",
    "\n",
    "# decoder\n",
    "# mean field decoder = pixel-independent Bernoulli distribution\n",
    "decoder = tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "    tfkl.Reshape([1, 1, encoded_size]),\n",
    "    tfkl.Conv2DTranspose(2 * base_depth, 7, strides=1,\n",
    "                         padding='valid', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2DTranspose(2 * base_depth, 5, strides=1,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2DTranspose(2 * base_depth, 5, strides=2,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2DTranspose(base_depth, 5, strides=1,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2DTranspose(base_depth, 5, strides=2,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2DTranspose(base_depth, 5, strides=1,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    tfkl.Conv2D(filters=1, kernel_size=5, strides=1,\n",
    "                padding='same', activation=None),\n",
    "    tfkl.Flatten(),\n",
    "    tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits),\n",
    "])\n",
    "\n",
    "\n",
    "# build model\n",
    "vae = tfk.Model(inputs=encoder.inputs,\n",
    "                outputs=decoder(encoder.outputs[0]))\n",
    "\n",
    "\n",
    "# Reconstruction loss. KL loss calculated in encoder\n",
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "\n",
    "vae.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss=negative_log_likelihood)\n",
    "\n",
    "# training\n",
    "_ = vae.fit(train_dataset,\n",
    "            epochs=15,\n",
    "            validation_data=eval_dataset)\n",
    "\n",
    "\n",
    "# We'll just examine ten random digits.\n",
    "x = next(iter(eval_dataset))[0][:10]\n",
    "xhat = vae(x)\n",
    "assert isinstance(xhat, tfd.Distribution)\n",
    "\n",
    "# plot\n",
    "def display_imgs(x, y=None):\n",
    "  if not isinstance(x, (np.ndarray, np.generic)):\n",
    "    x = np.array(x)\n",
    "  plt.ioff()\n",
    "  n = x.shape[0]\n",
    "  fig, axs = plt.subplots(1, n, figsize=(n, 1))\n",
    "  if y is not None:\n",
    "    fig.suptitle(np.argmax(y, axis=1))\n",
    "  for i in range(n):\n",
    "    axs.flat[i].imshow(x[i].squeeze(), interpolation='none', cmap='gray')\n",
    "    axs.flat[i].axis('off')\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "  plt.ion()\n",
    "\n",
    "print('Originals:')\n",
    "display_imgs(x)\n",
    "\n",
    "print('Decoded Random Samples:')\n",
    "display_imgs(xhat.sample())\n",
    "\n",
    "print('Decoded Modes:')\n",
    "display_imgs(xhat.mode())\n",
    "\n",
    "print('Decoded Means:')\n",
    "display_imgs(xhat.mean())\n",
    "\n",
    "\n",
    "# Now, let's generate ten never-before-seen digits.\n",
    "z = prior.sample(10)\n",
    "xtilde = decoder(z)\n",
    "assert isinstance(xtilde, tfd.Distribution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-c4f24b27",
   "language": "python",
   "display_name": "PyCharm (MLinPhysics)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}