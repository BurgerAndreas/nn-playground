{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f45faf0",
   "metadata": {},
   "source": [
    "# Machine Learning in Fundamental Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfcd0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import Input, Sequential\n",
    "from tensorflow.python.keras.engine.functional import Functional\n",
    "from tensorflow.python.keras.engine.keras_tensor import KerasTensor\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Lambda, Conv2D, Conv2DTranspose, LeakyReLU, \\\n",
    "    GlobalMaxPooling2D, Reshape, Cropping2D, Dropout\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859dd21",
   "metadata": {},
   "source": [
    "## Exercise Sheet 11, Exercise 1\n",
    "You can find a dataset on the Moodle website (ising data.zip). These are some Monte Carlo samples\n",
    "of the standard ferromagnetic 2D Ising model on a square lattice of size 40x40 generated at different\n",
    "temperatures. For each temperature there are 1001 spin configurations. The factor indicates how\n",
    "close it is to the critical temperature (e.g. 1 corresponds to 0.1βc, βc = 1/(TckB)). It contains various\n",
    "numpy arrays so you can load the datasets using numpy.\n",
    "\n",
    "Build a GAN with a custom training loop (as in sheet 10) that aims at generating Ising samples\n",
    "below the critical temperature. Analyze the energy and magnetization of both the original dataset\n",
    "and the generated Ising samples.\n",
    "Note that in case you do not have a local environment where you can calculate on a GPU, you\n",
    "can calculate online on collab on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d45114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsingData:\n",
    "    \"\"\"Data class already implemented for you and ready to use.\"\"\"\n",
    "    def __init__(self, data_fraction=1./10):\n",
    "        self.x_train = np.load('./ising_data/ising_11.npy')\n",
    "        self.reshape_to_color_channel()\n",
    "\n",
    "    def reshape_to_color_channel(self):\n",
    "        self.x_train = self.x_train[:, :, :, np.newaxis]\n",
    "        #self.x_test = self.x_test[:, :, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad38cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, data: IsingData, num_classes=10):\n",
    "        self.x_train = data.x_train\n",
    "        \n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shape = self.x_train[0].shape\n",
    "        self.input = Input(shape=self.input_shape)\n",
    "        self.output = None\n",
    "\n",
    "        self.latent_dim = 128\n",
    "        \n",
    "        self.discriminator = None\n",
    "        self.generator = None\n",
    "        \n",
    "        self.build_discriminator()\n",
    "        self.build_generator()\n",
    "\n",
    "        #self.discriminator.summary\n",
    "        #self.generator.summary\n",
    "        \n",
    "        self.d_optimizer = None\n",
    "        self.g_optimizer = None\n",
    "        self.loss_fn = None\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        \"\"\"Build the discriminator.\n",
    "        basically a CNN without max-pooling, but strided convolution instead\"\"\"\n",
    "        depth = 64\n",
    "        kernel = (3,3)\n",
    "        dropout = 0.4\n",
    "        self.discriminator = Sequential(\n",
    "            [\n",
    "                Input(shape=(40, 40, 1)),\n",
    "                Conv2D(depth, kernel, strides=(2, 2), padding=\"same\"),\n",
    "                LeakyReLU(alpha=0.2),\n",
    "                #Dropout(dropout) # combat overfitting\n",
    "                Conv2D(depth*2, kernel, strides=(2, 2), padding=\"same\"),\n",
    "                LeakyReLU(alpha=0.2),\n",
    "                #Dropout(dropout) # combat overfitting\n",
    "                GlobalMaxPooling2D(),\n",
    "                #Flatten()\n",
    "                Dense(1),\n",
    "            ],\n",
    "            name=\"discriminator\",\n",
    "        )\n",
    "        self.discriminator.summary()\n",
    "        \n",
    "    def build_generator(self):\n",
    "        \"\"\"Build the generator.\"\"\"\n",
    "        latent_dim = self.latent_dim\n",
    "        dim = 10\n",
    "        depth = 128\n",
    "        dropout = 0.4\n",
    "\n",
    "        self.generator = Sequential(\n",
    "            [\n",
    "                Input(shape=(latent_dim, )),\n",
    "                # We want to generate 128 coefficients to reshape into a 10x10x128 map\n",
    "                Dense(dim*dim*depth),\n",
    "                LeakyReLU(alpha=0.2),\n",
    "                #BatchNormalization(momentum=0.9)\n",
    "                #LeakyReLU(alpha=0.2)\n",
    "                Reshape((dim, dim, depth)),\n",
    "                #Dropout(dropout)\n",
    "                # upsampling\n",
    "                Conv2DTranspose(depth, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "                LeakyReLU(alpha=0.2),\n",
    "                #Dropout(0.05),\n",
    "                Conv2DTranspose(depth, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "                LeakyReLU(alpha=0.2),\n",
    "                #Conv2DTranspose(depth, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "                #LeakyReLU(alpha=0.2),\n",
    "                Conv2D(1, (10, 10), padding=\"same\", activation=\"sigmoid\"),\n",
    "                #Cropping2D(cropping=((20, 20), (20, 20)))\n",
    "            ],\n",
    "            name=\"generator\",\n",
    "        )\n",
    "        self.generator.summary()\n",
    "\n",
    "    \"\"\"\n",
    "        def build_discriminator(self):\n",
    "        #Build the discriminator.\n",
    "        #basically a CNN without max-pooling, but strided convolution instead\n",
    "        depth = 64\n",
    "        kernel = (3,3)\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(Conv2D(depth, kernel, strides=(2, 2), padding='same',\n",
    "                                         input_shape=[40, 40, 1]))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Conv2D(depth*2, kernel, strides=(2, 2), padding='same'))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        self.discriminator = model\n",
    "        self.discriminator.summary()\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        #The generator uses tf.keras.layers.Conv2DTranspose (upsampling) layers to produce\n",
    "        #an image from a seed (random noise). Start with a Dense layer that takes this seed\n",
    "        #as input, then upsample several times until you reach the desired image size of 28x28x1\n",
    "\n",
    "        latent_dim = self.latent_dim\n",
    "        dim = 10\n",
    "        depth = 128\n",
    "        kernel = (3,3)\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(Dense(dim*dim*depth, use_bias=False, input_shape=(latent_dim,)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        model.add(Reshape((dim, dim, depth)))\n",
    "        #assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
    "\n",
    "        model.add(Conv2DTranspose(128, kernel, strides=(1, 1), padding='same', use_bias=False))\n",
    "        #assert model.output_shape == (None, 7, 7, 128)\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        model.add(Conv2DTranspose(64, kernel, strides=(2, 2), padding='same', use_bias=False))\n",
    "        #assert model.output_shape == (None, 14, 14, 64)\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        model.add(Conv2DTranspose(1, kernel, strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "        #assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "        self.generator = model\n",
    "        self.generator.summary()\n",
    "        \"\"\"\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, batch_size, real_images):\n",
    "        \"\"\"GAN training step.\"\"\"\n",
    "        latent_dim = self.latent_dim\n",
    "        \n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim), mean=0.0, stddev=1.0)\n",
    "        #random_latent_vectors = tf.random.uniform(shape=(batch_size, latent_dim), minval=-1, maxval=1) \n",
    "        \n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "        #Map tensor elements to either 0 or 1\n",
    "        generated_images = tf.dtypes.cast(generated_images, tf.int8)\n",
    "        #Map tensor elements to either -1 or +1 (spins)\n",
    "        generated_images = 2*generated_images - tf.ones(shape=(batch_size, 40, 40, 1), dtype=tf.dtypes.int8)\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((real_images.shape[0], 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        #labels += 0.05 * tf.random.uniform(labels.shape)\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            print(\"discriminator input shape\", combined_images.shape)\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        # calc gradient\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        # apply gradient\n",
    "        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            print(\"generator input shape\", random_latent_vectors.shape)\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        return d_loss, g_loss, generated_images    \n",
    "\n",
    "\n",
    "    def calculate_magnetization_dist(self, samples):\n",
    "        \"\"\"Returns magnetization distribution of given input samples\"\"\"\n",
    "        magnetization_dist = np.zeros((100))\n",
    "        for sample in samples:\n",
    "            sample = sample.numpy()\n",
    "            magnetization = abs(np.mean(sample))\n",
    "            i = int(100*magnetization-1e-9)\n",
    "            magnetization_dist[i] += 1\n",
    "            \n",
    "        return magnetization_dist\n",
    "\n",
    "\n",
    "    def calculate_energy_dist(self, samples):\n",
    "        \"\"\"Returns energy distribution of given input samples\"\"\"\n",
    "        energy_dist = np.zeros((100))\n",
    "        for sample in samples:\n",
    "            sample = sample.numpy()\n",
    "            ener = 0\n",
    "            for i in range(40):\n",
    "                for j in range(40):\n",
    "                    sp1 = sample[(i+1)%40,j]\n",
    "                    sp2 = sample[i,(j+1)%40]\n",
    "                    ener += - sample[i,j] * (sp1+sp2)\n",
    "            #normalize energy\n",
    "            ener = (ener)/(2*2*40) \n",
    "            ener = (ener + 1)/2\n",
    "            i = int(100*ener-1e-9)\n",
    "            energy_dist[i] += 1\n",
    "        \n",
    "        return energy_dist\n",
    "\n",
    "\n",
    "    def plot_ising(self, real_sample, calc_sample):\n",
    "        \"\"\"Plots Monte-Carlo-samples in comparison to the generated samples\"\"\"\n",
    "        fig = plt.figure()\n",
    "\n",
    "        fig.set_figheight(5)\n",
    "        fig.set_figwidth(15)\n",
    "        gs = fig.add_gridspec(1, 2)\n",
    "        \n",
    "        ax = fig.add_subplot(gs[0, 0])\n",
    "        ax.imshow(real_sample)\n",
    "        ax.set_title('real_sample')\n",
    "        \n",
    "        ax = fig.add_subplot(gs[0, 1])\n",
    "        ax.imshow(calc_sample)\n",
    "        ax.set_title('calculated_sample')\n",
    "        \n",
    "        plt.tight_layout\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_dist(self, magnetization_real, magnetization_calc, energy_real, energy_calc):\n",
    "        \"\"\"Plots distribution functions of Monte-Carlo-samples in comparison to the generated samples\"\"\"\n",
    "        \n",
    "        mag_x = np.linspace(0,1,100)\n",
    "        energ_x = np.linspace(-1,1,100)\n",
    "        \n",
    "        fig = plt.figure()\n",
    "\n",
    "        fig.set_figheight(5)\n",
    "        fig.set_figwidth(15)\n",
    "        gs = fig.add_gridspec(2, 2, hspace=1)\n",
    "        \n",
    "        ax = fig.add_subplot(gs[0, 0])\n",
    "        ax.plot(mag_x, magnetization_real)\n",
    "        ax.set_title('magnetization_real')\n",
    "        \n",
    "        ax = fig.add_subplot(gs[0, 1])\n",
    "        ax.plot(mag_x, magnetization_calc)\n",
    "        ax.set_title('magnetization_calc')\n",
    "        \n",
    "        ax = fig.add_subplot(gs[1, 0])\n",
    "        ax.plot(energ_x, energy_real)\n",
    "        ax.set_title('energy_real')\n",
    "        \n",
    "        ax = fig.add_subplot(gs[1, 1])\n",
    "        ax.plot(energ_x, energy_calc)\n",
    "        ax.set_title('energy_calc')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def train_gan(self, batch_size=128, epochs=20):\n",
    "        \"\"\"training loop\"\"\"\n",
    "        #Prepare dataset\n",
    "        self.train_dataset = tf.data.Dataset.from_tensor_slices(self.x_train)\n",
    "        self.train_dataset = self.train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "        # Instantiate an optimizer.\n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        # Instantiate a loss function.\n",
    "        self.loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        \n",
    "        #self.train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "        #self.val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        # save images for plotting\n",
    "        real_images_f = None\n",
    "        generated_images_f = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "            start_time_epoch = time.time()\n",
    "\n",
    "            # Iterate over the batches of the dataset.\n",
    "            for step, real_images in enumerate(self.train_dataset):\n",
    "                d_loss, g_loss, generated_images = self.train_step(batch_size, real_images)\n",
    "                \n",
    "                # Log for plotting\n",
    "                real_images_f = real_images\n",
    "                generated_images_f = generated_images\n",
    "                \n",
    "                # Log every 200 batches.\n",
    "                if step % 200 == 0:\n",
    "                    print(\n",
    "                        \"Discriminator loss (for one batch) at step %d: %.4f\"\n",
    "                        % (step, float(d_loss))\n",
    "                    )\n",
    "                    print(\n",
    "                        \"Generator loss (for one batch) at step %d: %.4f\"\n",
    "                        % (step, float(g_loss))\n",
    "                    )\n",
    "                    print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "            \n",
    "            \n",
    "            print(\"Time taken: %.2fs\" % (time.time() - start_time_epoch))\n",
    "            \n",
    "            self.plot_ising(real_images_f[0], generated_images_f[0])\n",
    "            \n",
    "            magnetization_real = self.calculate_magnetization_dist(real_images_f)\n",
    "            magnetization_calc = self.calculate_magnetization_dist(generated_images_f)\n",
    "                \n",
    "            energy_real = self.calculate_energy_dist(real_images_f)\n",
    "            energy_calc = self.calculate_energy_dist(generated_images_f)\n",
    "            print('')\n",
    "            \n",
    "            self.plot_dist(magnetization_real, magnetization_calc, energy_real, energy_calc)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        print(\"\\nTotal time taken: %.2fs\" % (time.time() - start_time))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "284a57a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 20, 20, 64)        640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 20, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 10, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 74,625\n",
      "Trainable params: 74,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 12800)             1651200   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 20, 20, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 40, 40, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 40, 40, 1)         12801     \n",
      "=================================================================\n",
      "Total params: 2,188,545\n",
      "Trainable params: 2,188,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Start of epoch 0\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GAN.train_step of <tensorflow.python.eager.function.TfMethodTarget object at 0x000002B47875FC10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method GAN.train_step of <tensorflow.python.eager.function.TfMethodTarget object at 0x000002B47875FC10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "discriminator input shape (256, 40, 40, 1)\n",
      "generator input shape (128, 128)\n",
      "discriminator input shape (256, 40, 40, 1)\n",
      "generator input shape (128, 128)\n",
      "Discriminator loss (for one batch) at step 0: 0.6781\n",
      "Generator loss (for one batch) at step 0: 0.6653\n",
      "Seen so far: 128 samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-16-087f1e2c9f7c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mising_gan\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mGAN\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mising_gan\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_gan\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m128\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m30\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-15-d9ba8f4cd381>\u001B[0m in \u001B[0;36mtrain_gan\u001B[1;34m(self, batch_size, epochs)\u001B[0m\n\u001B[0;32m    302\u001B[0m             \u001B[1;31m# Iterate over the batches of the dataset.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    303\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mstep\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreal_images\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_dataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 304\u001B[1;33m                 \u001B[0md_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mg_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgenerated_images\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreal_images\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    305\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    306\u001B[0m                 \u001B[1;31m# Log for plotting\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    778\u001B[0m       \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    779\u001B[0m         \u001B[0mcompiler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"nonXla\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 780\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    781\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    782\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    805\u001B[0m       \u001B[1;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    806\u001B[0m       \u001B[1;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 807\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=not-callable\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    808\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    809\u001B[0m       \u001B[1;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2827\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2828\u001B[0m       \u001B[0mgraph_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2829\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_filtered_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2830\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2831\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_filtered_call\u001B[1;34m(self, args, kwargs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1841\u001B[0m       \u001B[0;31m`\u001B[0m\u001B[0margs\u001B[0m\u001B[0;31m`\u001B[0m \u001B[1;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1842\u001B[0m     \"\"\"\n\u001B[1;32m-> 1843\u001B[1;33m     return self._call_flat(\n\u001B[0m\u001B[0;32m   1844\u001B[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001B[0;32m   1845\u001B[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1921\u001B[0m         and executing_eagerly):\n\u001B[0;32m   1922\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1923\u001B[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0m\u001B[0;32m   1924\u001B[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0;32m   1925\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    543\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0m_InterpolateFunctionError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    544\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcancellation_manager\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 545\u001B[1;33m           outputs = execute.execute(\n\u001B[0m\u001B[0;32m    546\u001B[0m               \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    547\u001B[0m               \u001B[0mnum_outputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_outputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\MLinPhysics38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     57\u001B[0m   \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 59\u001B[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[0;32m     60\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[0;32m     61\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "data = IsingData()\n",
    "\n",
    "ising_gan = GAN(data)\n",
    "\n",
    "ising_gan.train_gan(batch_size=128, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017cd40a",
   "metadata": {},
   "source": [
    "The performance of the native Keras method is consistently slightly better, but the training takes approximately the same time (it varies a little bit for both models)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7cc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d25179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cafba2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-c4f24b27",
   "language": "python",
   "display_name": "PyCharm (MLinPhysics)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}